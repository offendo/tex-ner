{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c25997b-c3f5-4b88-b661-2820ab26662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import openai\n",
    "import bs4\n",
    "from more_itertools import chunked, flatten\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB2\n",
    "from pathlib import Path\n",
    "openai.api_key = open('/Users/offendo/.openai-key', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cfcd0-0c20-41e0-a267-68f60d048d54",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7efdc2c7-800a-4a8d-9b64-81eb591b774d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0da8ab2d-d3d8-47b7-a433-06d9919f3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_xml(tokens, tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tags to XML format as a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens (words).\n",
    "        tags (list): List of BIO tags corresponding to each token.\n",
    "        \n",
    "    Returns:\n",
    "        list: XML representation of the tagged entities as a list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    xml_tokens = []\n",
    "    current_entity = None\n",
    "    current_entity_tokens = []\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):  # Beginning of a new entity\n",
    "            # Close the previous entity tag if there is one\n",
    "            if current_entity:\n",
    "                xml_tokens.append(f\"<{current_entity}>\")\n",
    "                xml_tokens.extend(current_entity_tokens)\n",
    "                xml_tokens.append(f\"</{current_entity}>\")\n",
    "            # Start a new entity\n",
    "            current_entity = tag[2:]\n",
    "            current_entity_tokens = [token]\n",
    "        elif tag.startswith(\"I-\") and current_entity == tag[2:]:  # Inside the same entity\n",
    "            current_entity_tokens.append(token)\n",
    "        else:  # Outside any entity\n",
    "            # Close the previous entity tag if there is one\n",
    "            if current_entity:\n",
    "                xml_tokens.append(f\"<{current_entity}>\")\n",
    "                xml_tokens.extend(current_entity_tokens)\n",
    "                xml_tokens.append(f\"</{current_entity}>\")\n",
    "                current_entity = None\n",
    "                current_entity_tokens = []\n",
    "            # Add the token outside any tag\n",
    "            xml_tokens.append(token)\n",
    "\n",
    "    # Close any remaining entity tag at the end\n",
    "    if current_entity:\n",
    "        xml_tokens.append(f\"<{current_entity}>\")\n",
    "        xml_tokens.extend(current_entity_tokens)\n",
    "        xml_tokens.append(f\"</{current_entity}>\")\n",
    "        \n",
    "    return xml_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1a7b73f-508c-4297-94f0-771ce0aff76e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_iob_to_xml(json_file, tokenizer, maxlen: int = None):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    labels = ['definition', 'theorem', 'example', 'proof']\n",
    "    examples = []\n",
    "\n",
    "    maxlen = maxlen or 999999999\n",
    "    for chunk in chunked(data['iob_tags'], n=maxlen):\n",
    "        tokens = []\n",
    "        joined_tags = []\n",
    "        for token, tags in chunk:\n",
    "            filtered_tags = [t.replace('B-', '').replace('I-', '') for t in tags]\n",
    "            filtered_tags = [t for t in filtered_tags if t in labels]\n",
    "            begin = any([t.startswith('B-') for t in tags if t.strip('B').strip('I').strip('-') in labels])\n",
    "            joined = '-'.join(sorted(filtered_tags)) or 'O'\n",
    "            if joined != 'O':\n",
    "                joined = 'B-' + joined if begin else 'I-' + joined\n",
    "            if len(joined_tags) > 1 and joined.replace('B-', '') == joined_tags[-1].replace('B-', '').replace('I-', ''):\n",
    "                joined = joined.replace('B-', 'I-')\n",
    "            joined_tags.append(joined)\n",
    "            tokens.append(token)\n",
    "\n",
    "        xml_result = bio_to_xml(tokens, joined_tags)\n",
    "        inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "        response = tokenizer.convert_tokens_to_string(xml_result)\n",
    "        examples.append({'prompt': inputs, 'completion': response})\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c63740-7fd9-4b65-9474-dd47dc98d382",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_iob_to_xml_name_ref(json_file, tokenizer, maxlen: int = None, labels: tuple[str] | list[str] = ('name', 'ref')):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    examples = []\n",
    "\n",
    "    maxlen = maxlen or 999999999\n",
    "    for chunk in chunked(data['iob_tags'], n=maxlen):\n",
    "        tokens = []\n",
    "        joined_tags = []\n",
    "        for token, tags in chunk:\n",
    "            filtered_tags = [t.replace('B-', '').replace('I-', '') for t in tags]\n",
    "            filtered_tags = [t for t in filtered_tags if t in labels]\n",
    "            begin = any([t.startswith('B-') for t in tags if t.strip('B').strip('I').strip('-') in labels])\n",
    "            joined = '-'.join(sorted(filtered_tags)) or 'O'\n",
    "            if joined != 'O':\n",
    "                joined = 'B-' + joined if begin else 'I-' + joined\n",
    "            if len(joined_tags) > 1 and joined.replace('B-', '') == joined_tags[-1].replace('B-', '').replace('I-', ''):\n",
    "                joined = joined.replace('B-', 'I-')\n",
    "            joined_tags.append(joined)\n",
    "            tokens.append(token)\n",
    "\n",
    "        xml_result = bio_to_xml(tokens, joined_tags)\n",
    "        inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "        response = tokenizer.convert_tokens_to_string(xml_result)\n",
    "        examples.append({'prompt': inputs, 'completion': response})\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71d674f4-d047-42a5-a69d-578ff63eeda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bio_to_entities(tokens, tags, tokenizer):\n",
    "    entities = {}\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            # Start of a new entity\n",
    "            if current_entity and current_label:\n",
    "                # Add the previous entity to the result\n",
    "                entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "                entities.setdefault(current_label, []).append(entity_str)\n",
    "            current_entity = [token]\n",
    "            current_label = tag[2:]\n",
    "        elif tag.startswith(\"I-\") and current_label == tag[2:]:\n",
    "            # Continuation of the current entity\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            # End of an entity\n",
    "            if current_entity and current_label:\n",
    "                entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "                entities.setdefault(current_label, []).append(entity_str)\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "            if tag.startswith(\"B-\"):\n",
    "                current_entity = [token]\n",
    "                current_label = tag[2:]\n",
    "\n",
    "    # Add the last entity if there is one\n",
    "    if current_entity and current_label:\n",
    "        entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "        entities.setdefault(current_label, []).append(entity_str)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cea011d5-c938-4752-98a7-931c96bfdc14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_iob_to_json(json_file, tokenizer, maxlen: int = None):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = []\n",
    "    maxlen = maxlen or 9999999999\n",
    "    for chunk in chunked(data['iob_tags'], n=maxlen):\n",
    "        entities = dict(definition=[], theorem=[], example=[], proof=[])\n",
    "        current_def = []\n",
    "        current_theorem = []\n",
    "        current_example = []\n",
    "        current_proof = []\n",
    "        tokens = []\n",
    "        for token, tags in chunk:\n",
    "            tokens.append(token)\n",
    "            for tag in tags:\n",
    "                if tag.startswith('B-') and 'definition' in tag:\n",
    "                    if current_def:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_def)\n",
    "                        entities['definition'].append(entity_str)\n",
    "                    current_def = [token]\n",
    "                if tag.startswith('I-') and 'definition' in tag:\n",
    "                    current_def.append(token)\n",
    "                    \n",
    "                if tag.startswith('B-') and 'theorem' in tag:\n",
    "                    if current_theorem:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_theorem)\n",
    "                        entities['theorem'].append(entity_str)\n",
    "                    current_theorem = [token]\n",
    "                if tag.startswith('I-') and 'theorem' in tag:\n",
    "                    current_theorem.append(token)\n",
    "                    \n",
    "                if tag.startswith('B-') and 'example' in tag:\n",
    "                    if current_example:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_example)\n",
    "                        entities['example'].append(entity_str)\n",
    "                    current_example = [token]\n",
    "                if tag.startswith('I-') and 'example' in tag:\n",
    "                    current_example.append(token)    \n",
    "                    \n",
    "                if tag.startswith('B-') and 'proof' in tag:\n",
    "                    if current_proof:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_proof)\n",
    "                        entities['proof'].append(entity_str)\n",
    "                    current_proof = [token]\n",
    "                if tag.startswith('I-') and 'proof' in tag:\n",
    "                    current_proof.append(token)\n",
    "    \n",
    "        if current_def:\n",
    "            entities['definition'].append(tokenizer.convert_tokens_to_string(current_def))\n",
    "        if current_theorem:\n",
    "            entities['theorem'].append(tokenizer.convert_tokens_to_string(current_theorem))\n",
    "        if current_example:\n",
    "            entities['example'].append(tokenizer.convert_tokens_to_string(current_example))\n",
    "        if current_proof:\n",
    "            entities['proof'].append(tokenizer.convert_tokens_to_string(current_proof))\n",
    "            \n",
    "        inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "        response = json.dumps(entities)\n",
    "        example = {'prompt': inputs, 'completion': response}\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55f030be-1165-412b-8d4f-a363ed8af749",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_iob_to_json_name_ref(json_file, tokenizer, maxlen: int = None, ):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = []\n",
    "    maxlen = maxlen or 9999999999\n",
    "    for chunk in chunked(data['iob_tags'], n=maxlen):\n",
    "        entities = dict(name=[], reference=[])\n",
    "        current_name = []\n",
    "        current_reference = []\n",
    "        tokens = []\n",
    "        for token, tags in chunk:\n",
    "            tokens.append(token)\n",
    "            for tag in tags:\n",
    "                if tag.startswith('B-') and 'name' in tag:\n",
    "                    if current_name:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_name)\n",
    "                        entities['name'].append(entity_str)\n",
    "                    current_name = [token]\n",
    "                if tag.startswith('I-') and 'name' in tag:\n",
    "                    current_name.append(token)\n",
    "                    \n",
    "                if tag.startswith('B-') and 'reference' in tag:\n",
    "                    if current_reference:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_reference)\n",
    "                        entities['reference'].append(entity_str)\n",
    "                    current_reference = [token]\n",
    "                if tag.startswith('I-') and 'reference' in tag:\n",
    "                    current_reference.append(token)\n",
    "                    \n",
    "    \n",
    "        if current_name:\n",
    "            entities['name'].append(tokenizer.convert_tokens_to_string(current_name))\n",
    "        if current_reference:\n",
    "            entities['reference'].append(tokenizer.convert_tokens_to_string(current_reference))\n",
    "            \n",
    "        inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "        response = json.dumps(entities)\n",
    "        example = {'prompt': inputs, 'completion': response}\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eaff0f45-2067-458b-9446-bd7f9b693b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/train/'):\n",
    "    trains.extend(convert_iob_to_json(f'./data/roberta-base-new-data/train/{file}', tokenizer, 1024))\n",
    "\n",
    "vals = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/val/'):\n",
    "    vals.extend(convert_iob_to_json(f'./data/roberta-base-new-data/val/{file}', tokenizer, 1024))\n",
    "\n",
    "test = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/test/'):\n",
    "    test.extend(convert_iob_to_json(f'./data/roberta-base-new-data/test/{file}', tokenizer, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f69698d4-8626-415c-aea2-621192341bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(trains)\n",
    "val = pd.DataFrame(vals)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "train.to_json('./data/openai/train.jsonl', orient='records', lines=True)\n",
    "val.to_json('./data/openai/val.jsonl', orient='records', lines=True)\n",
    "test.to_json('./data/openai/test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56271b1-aa42-4944-83a7-c4997cf5b3ff",
   "metadata": {},
   "source": [
    "# Few-shot Prompting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1544d203-93aa-4392-b5f9-092e069f96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bio_tags(sentence, entities):\n",
    "    # Tokenize the sentence by splitting on spaces and punctuation, while keeping the tokens intact\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence, re.UNICODE)\n",
    "    tags = [[] for _ in range(len(tokens))]\n",
    "\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        for entity in entity_list:\n",
    "            # Tokenize the entity name in the same way to match against tokens\n",
    "            entity_tokens = re.findall(r\"\\w+|[^\\w\\s]\", entity, re.UNICODE)\n",
    "            entity_len = len(entity_tokens)\n",
    "            # Search for the entity tokens in the sentence tokens\n",
    "            for i in range(len(tokens) - entity_len + 1):\n",
    "                if tokens[i:i + entity_len] == entity_tokens:\n",
    "                    for j in range(entity_len):\n",
    "                        tags[i + j].append(f\"{entity_type}\")\n",
    "                    break  # Move to the next entity once found\n",
    "    joined_tags = ['-'.join(sorted(set(t))) if len(t) > 0 else 'O' for t in tags]\n",
    "    return tokens, joined_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f6f348d-1e74-4435-b602-dd662744f27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_model_output(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df['predictions'] = df['predictions'].apply(lambda x: json.loads(x))\n",
    "    df['completion'] = df['completion'].apply(lambda x: json.loads(x.replace(\"<|endoftext|>\", \"\")))\n",
    "    preds = {}\n",
    "    for p in df.predictions:\n",
    "        for k in p.keys():\n",
    "            preds[k] = preds.get(k, []) + p[k]\n",
    "    golds = {}\n",
    "    for g in df.completion:\n",
    "        for k in g.keys():\n",
    "            golds[k] = golds.get(k, []) + g[k]\n",
    "    tokens, gold_tags = generate_bio_tags(' '.join(df.prompt), golds)\n",
    "    tokens, pred_tags = generate_bio_tags(' '.join(df.prompt), preds)\n",
    "    labels = [t for t in set(gold_tags) if t != 'O']\n",
    "    p, r, f, _ = precision_recall_fscore_support(gold_tags, pred_tags, average=None, labels=labels)\n",
    "    mp, mr, mf, _ = precision_recall_fscore_support(gold_tags, pred_tags, average='micro', labels=labels)\n",
    "\n",
    "    result = pd.DataFrame({'tokens': tokens, 'golds': gold_tags, 'preds': pred_tags})\n",
    "    metrics = dict(\n",
    "        precision=dict(zip(labels, p)),\n",
    "        recall=dict(zip(labels, r)),\n",
    "        f1=dict(zip(labels, f)),\n",
    "        micro_precision=mp,\n",
    "        micro_recall=mr,\n",
    "        micro_f1=mf,\n",
    "    )\n",
    "    return metrics, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f35eb9ff-18f5-4caf-8404-706a1fc457f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.13625,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.7440476190476191,\n",
      "        'proof': 0.46064889174429813,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.14026402640264027},\n",
      " 'micro_f1': 0.304661079253051,\n",
      " 'micro_precision': 0.7754491017964071,\n",
      " 'micro_recall': 0.18956999085086917,\n",
      " 'precision': {'definition': 1.0,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.6157635467980296,\n",
      "               'proof': 0.8056179775280898,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6343283582089553},\n",
      " 'recall': {'definition': 0.07310529845741114,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.9398496240601504,\n",
      "            'proof': 0.3225371120107962,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.07884972170686456}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-3.5-turbo.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "77ab2d69-201b-411d-8662-100a71305efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.42953472690492245,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.8632478632478633,\n",
      "        'proof': 0.7035126504544338,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.4852492370295015},\n",
      " 'micro_f1': 0.541474890048072,\n",
      " 'micro_precision': 0.6138682745825603,\n",
      " 'micro_recall': 0.48435498627630375,\n",
      " 'precision': {'definition': 0.43186440677966104,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.7748917748917749,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.5371621621621622},\n",
      " 'recall': {'definition': 0.4272300469483568,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.7593984962406015,\n",
      "            'proof': 0.644174538911381,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.4424860853432282}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4-turbo.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "748fcfbe-d5ad-4057-95a0-058a4c053535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5944418276024493,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.0,\n",
      "        'proof': 0.3002645502645503,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.38411910669975186},\n",
      " 'micro_f1': 0.3757498404594767,\n",
      " 'micro_precision': 0.6210970464135022,\n",
      " 'micro_recall': 0.26935041171088747,\n",
      " 'precision': {'definition': 0.9984177215189873,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.0,\n",
      "               'proof': 0.5667915106117354,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.41302027748132336},\n",
      " 'recall': {'definition': 0.4232059020791415,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.0,\n",
      "            'proof': 0.20422852001799371,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.3589981447124304}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4o-mini.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f25b0b5-4681-4f22-86b7-98ba62753643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.6274199920979849,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.5859030837004405,\n",
      "        'proof': 0.6983471074380165,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.49830508474576274},\n",
      " 'micro_f1': 0.5934329660739609,\n",
      " 'micro_precision': 0.7347379794705564,\n",
      " 'micro_recall': 0.4977127172918573,\n",
      " 'precision': {'definition': 0.7634615384615384,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.4143302180685358,\n",
      "               'proof': 0.8198908429351122,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6372832369942196},\n",
      " 'recall': {'definition': 0.5325285043594903,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 1.0,\n",
      "            'proof': 0.6081871345029239,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.4090909090909091}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4o.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dc45551d-ed66-49cc-9962-5f5e80c53abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5504241281809613,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.18815331010452963,\n",
      "        'proof': 0.6437793427230047,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.2773837667454689},\n",
      " 'micro_f1': 0.4940991345397325,\n",
      " 'micro_precision': 0.8718186024988431,\n",
      " 'micro_recall': 0.34473924977127174,\n",
      " 'precision': {'definition': 0.9255150554675119,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.17532467532467533,\n",
      "               'proof': 0.9257383966244725,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.9214659685863874},\n",
      " 'recall': {'definition': 0.39168343393695504,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.20300751879699247,\n",
      "            'proof': 0.49347728295096716,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.16326530612244897}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-mini.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c901e0d5-20de-461a-a157-32052579886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.1524609843937575,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.0,\n",
      "        'proof': 0.35690968443960824,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.23993558776167473},\n",
      " 'micro_f1': 0.2423477437677501,\n",
      " 'micro_precision': 0.8797250859106529,\n",
      " 'micro_recall': 0.14053064958828912,\n",
      " 'precision': {'definition': 0.7257142857142858,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.0,\n",
      "               'proof': 0.9213483146067416,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.9085365853658537},\n",
      " 'recall': {'definition': 0.085177733065057,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.0,\n",
      "            'proof': 0.2213225371120108,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.13821892393320964}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-mini.5shot.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "169dabf1-1f37-469d-8cd9-1dc1ea26b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.6172506738544474,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.7208672086720868,\n",
      "        'proof': 0.6985842985842986,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.46497584541062803},\n",
      " 'micro_f1': 0.5900506678949793,\n",
      " 'micro_precision': 0.7958993476234856,\n",
      " 'micro_recall': 0.4688014638609332,\n",
      " 'precision': {'definition': 0.9346938775510204,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.5635593220338984,\n",
      "               'proof': 0.8164861612515042,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6660899653979239},\n",
      " 'recall': {'definition': 0.4607645875251509,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 1.0,\n",
      "            'proof': 0.6104363472784525,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.35714285714285715}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-preview.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "11acfdc0-4d46-4d18-92b3-9f82410b7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-o1-preview.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-o1-preview.val.outputs.csv > results/openai-o1-preview.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "76374de7-95a8-4b5c-a24f-28ff748c0cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5888704318936877,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.3878787878787879,\n",
      "        'proof': 0.8414690841469085,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.6076086956521739},\n",
      " 'micro_f1': 0.664956168484071,\n",
      " 'micro_precision': 0.7996914373875033,\n",
      " 'micro_recall': 0.5690759377859104,\n",
      " 'precision': {'definition': 0.7731733914940022,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.8706108706108706,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.7335958005249343},\n",
      " 'recall': {'definition': 0.4755197853789403,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.24060150375939848,\n",
      "            'proof': 0.8142150247413406,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.5185528756957328}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-preview.5shot.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "658d8e67-ffc8-4a44-ba00-9c72c62944ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-o1-preview.5shot.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-o1-preview.5shot.val.outputs.csv > results/openai-o1-preview.5shot.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce8745-a314-4edf-aca4-33e9d20dd9bb",
   "metadata": {},
   "source": [
    "# Base Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb0a529-9020-4be1-b21a-398a586d6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "INST = \"\"\"In the following LaTeX document, extract entities of the following types and return them in JSON output.\n",
    "1. definition\n",
    "2. theorem\n",
    "3. proof\n",
    "4. example\n",
    "Your output should be a single JSON with 4 keys corresponding to the 4 entity types above. Spans may be part of multiple entities. Do not hallucinate text.\n",
    "\"\"\"\n",
    "SYST = \"You are an expert mathematician who is fluent in reading LaTeX and extracting information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74e5d583-f54b-4a0b-90bf-0fe20d936a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example_message(prompt, completion, instruction, system):\n",
    "    messages = [{'role': 'system', 'content': system}]\n",
    "    messages += [{'role': 'user', 'content': instruction + '\\n\\n' + prompt}]\n",
    "    messages += [{'role': 'assistant', 'content': completion}]\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b73f73cc-1e42-402b-8b8a-4d93bbf16c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ms = train.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "val_ms = val.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "test_ms = test.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f2b4c3e-925e-46d1-bd8c-16830b2c6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(train_ms).to_json('./data/openai/texner-finetune.train.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(val_ms).to_json('./data/openai/texner-finetune.val.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(test_ms).to_json('./data/openai/texner-finetune.test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "561565cd-5eb1-4746-85fd-7deb5539d8ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_error_check(dataset):\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "    \n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "    \n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cef9f25c-8b3b-4a1f-9193-f318f5c5aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "No errors found\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "format_error_check(train_ms)\n",
    "format_error_check(val_ms)\n",
    "format_error_check(test_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c0247f8-c6c6-4768-a254-481f1d6c8826",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 128, 2288\n",
      "mean / median: 1543.8515625, 1798.5\n",
      "p5 / p95: 472.00000000000006, 2107.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 17, 1232\n",
      "mean / median: 704.8984375, 771.5\n",
      "p5 / p95: 160.9, 1045.1\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~197613 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~592839 tokens\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in train_ms:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(train_ms)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db4bce8b-34a6-4072-acf0-7be5c657dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "train_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "val_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "test_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.test.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31c08d1e-f0de-4393-9dc0-eda40f6d3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\",\n",
    "  validation_file=val_file.id,\n",
    "  seed=1299874447\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d93b6f76-c87b-46b7-92c1-3aa9c0fec1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-wkFvqSR6uSjhMb7OPgJvB2ky', created_at=1731131624, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-WEn9XU2tXbsjLzHzQeX2qlo4', result_files=[], seed=1299874447, status='validating_files', trained_tokens=None, training_file='file-UhPhFQSBWZJ4C7tzbEQnPipq', validation_file='file-wajDXEn6MriZp3hf2hn6JpFA', estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d628a0f-135b-45ee-8d21-1f81617914f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fine-tuning, find this value from the openai dashboard\n",
    "# MODEL = \"ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp::ARXGQBef\"\n",
    "MODEL = \"ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp::ARYwAxu8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35890d06-c288-4fdc-8742-80c5dd9efb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:01<00:00,  8.75s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('./response_schema.json', 'r') as f:\n",
    "    schema = json.load(f)\n",
    "preds = []\n",
    "for messages in tqdm(val_ms):\n",
    "    system, user, assistant = messages['messages']\n",
    "    completion = client.chat.completions.create(\n",
    "      model=MODEL,\n",
    "      messages=[system, user],\n",
    "      response_format=schema,\n",
    "    )\n",
    "    preds.append(json.loads(completion.choices[0].message.content))\n",
    "val['predictions'] = [json.dumps(p) for p in preds]\n",
    "val.to_json('./results/openai-4o-finetuned.val.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93704419-e937-4406-9e83-b296774bc5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.7272727272727273,\n",
      "        'definition-theorem': 0.24561403508771928,\n",
      "        'example': 0.42696629213483145,\n",
      "        'proof': 0.810062893081761,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.6144688644688645},\n",
      " 'micro_f1': 0.6968296614723267,\n",
      " 'micro_precision': 0.8442708333333333,\n",
      " 'micro_recall': 0.5932296431838975,\n",
      " 'precision': {'definition': 0.9839816933638444,\n",
      "               'definition-theorem': 1.0,\n",
      "               'example': 0.8444444444444444,\n",
      "               'proof': 0.9189497716894978,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6066907775768535},\n",
      " 'recall': {'definition': 0.5767940979208585,\n",
      "            'definition-theorem': 0.14,\n",
      "            'example': 0.2857142857142857,\n",
      "            'proof': 0.7242465137201979,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.6224489795918368}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.val.jsonl')\n",
    "pprint(metrics)\n",
    "outputs.to_csv(f'results/openai-4o-finetuned.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.val.outputs.csv > results/openai-4o-finetuned.val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc6d2871-83a0-4852-8b60-51aca80abd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [01:34<00:00,  8.57s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for messages in tqdm(test_ms):\n",
    "    system, user, assistant = messages['messages']\n",
    "    completion = client.chat.completions.create(\n",
    "      model=MODEL,\n",
    "      messages=[system, user],\n",
    "      response_format=schema,\n",
    "    )\n",
    "    preds.append(completion.choices[0].message.content)\n",
    "test['predictions'] = [p for p in preds]\n",
    "test.to_json('./results/openai-4o-finetuned.test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31cd99dd-40d2-42c0-a6d1-49be6ad38c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.7588163761653831,\n",
      "        'example': 0.3408360128617363,\n",
      "        'proof': 0.8524002640762048,\n",
      "        'theorem': 0.7233453337025755},\n",
      " 'micro_f1': 0.7937351904294053,\n",
      " 'micro_precision': 0.8782452999104745,\n",
      " 'micro_recall': 0.7240615773935049,\n",
      " 'precision': {'definition': 0.9322709163346613,\n",
      "               'example': 0.7969924812030075,\n",
      "               'proof': 0.9459912078710487,\n",
      "               'theorem': 0.6855643044619423},\n",
      " 'recall': {'definition': 0.6397812713602188,\n",
      "            'example': 0.2167689161554192,\n",
      "            'proof': 0.775660830758668,\n",
      "            'theorem': 0.7655334114888629}}\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.test.jsonl')\n",
    "pprint(metrics)\n",
    "outputs.to_csv(f'results/openai-4o-finetuned.test.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.test.outputs.csv > results/openai-4o-finetuned.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a84261cd-20ba-4374-9f5c-b71e814d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat './results/openai-4o-finetuned.test.jsonl' './results/openai-4o-finetuned.val.jsonl'  > './results/openai-4o-finetuned.both.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a877db8-6785-45bc-a252-87c98bc7f083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.7433774834437086,\n",
      "        'definition-theorem': 0.22105263157894736,\n",
      "        'example': 0.36,\n",
      "        'proof': 0.8408560845109069,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.6823123382226057},\n",
      " 'micro_f1': 0.7582223888993062,\n",
      " 'micro_precision': 0.8628371457835439,\n",
      " 'micro_recall': 0.6762325239146432,\n",
      " 'precision': {'definition': 0.9563365282215123,\n",
      "               'definition-theorem': 0.525,\n",
      "               'example': 0.8089887640449438,\n",
      "               'proof': 0.9387348751723081,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6565924941879774},\n",
      " 'recall': {'definition': 0.6079891672308734,\n",
      "            'definition-theorem': 0.14,\n",
      "            'example': 0.2315112540192926,\n",
      "            'proof': 0.7614610510622437,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.7101293103448276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.both.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "106a0b2c-2997-4789-9ade-a04b10cdb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-4o-finetuned.both.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.both.outputs.csv > results/openai-4o-finetuned.both.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c72bd8a-00c2-4b9c-ac89-7f2e304c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "def seqeval_score(outputs):\n",
    "    golds = []\n",
    "    current = None\n",
    "    for g in outputs.golds:\n",
    "        if g == 'O':\n",
    "            current = None\n",
    "            golds.append('O')\n",
    "        elif g == current:\n",
    "            golds.append(f'I-{g}')\n",
    "        else:\n",
    "            current = g\n",
    "            golds.append(f\"B-{g}\")\n",
    "    \n",
    "    preds = []\n",
    "    current = None\n",
    "    for p in outputs.preds:\n",
    "        if p == 'O':\n",
    "            current = None\n",
    "            preds.append('O')\n",
    "        elif p == current:\n",
    "            preds.append(f'I-{p}')\n",
    "        else:\n",
    "            current = p\n",
    "            preds.append(f\"B-{p}\")\n",
    "    ps = [chunk for chunk in chunked(preds, n=1024)]\n",
    "    gs = [chunk for chunk in chunked(golds, n=1024)]\n",
    "    return f1_score(gs, ps, mode=None, average='micro', scheme=IOB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ca670-ab5c-4920-8fe5-a9ece5752b7b",
   "metadata": {},
   "source": [
    "# Name only Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5f9d1636-7cbc-4eac-b426-13bf0fabbe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "base_dir = './data/roberta-base-new-data/'\n",
    "for file in os.listdir(f'{base_dir}/train'):\n",
    "    trains.extend(convert_iob_to_xml_name_ref(f'{base_dir}/train/{file}', tokenizer, 512, labels=('name',)))\n",
    "\n",
    "vals = []\n",
    "for file in os.listdir(f'{base_dir}/val/'):\n",
    "    vals.extend(convert_iob_to_xml_name_ref(f'{base_dir}/val/{file}', tokenizer, 512, labels=('name',)))\n",
    "\n",
    "test = []\n",
    "for file in os.listdir(f'{base_dir}/test/'):\n",
    "    test.extend(convert_iob_to_xml_name_ref(f'{base_dir}/test/{file}', tokenizer, 512, labels=('name',)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7fc9d3cd-51b1-4d1a-8b5c-f117c3a7fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(trains)\n",
    "val = pd.DataFrame(vals)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "train.to_json('./data/openai/train-name.jsonl', orient='records', lines=True)\n",
    "val.to_json('./data/openai/val-name.jsonl', orient='records', lines=True)\n",
    "test.to_json('./data/openai/test-name.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "254c6c49-c36a-4c3a-b6b5-d81c69422b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INST = \"\"\"In the following LaTeX snippet, extract every newly defined named entity in XML form. \n",
    "\"\"\"\n",
    "SYST = \"You are an expert mathematician who is fluent in reading LaTeX and extracting information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5e48f947-e673-4bb6-97c2-56918fed10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example_message(prompt, completion, instruction, system):\n",
    "    messages = [{'role': 'system', 'content': system}]\n",
    "    messages += [{'role': 'user', 'content': instruction + '\\n\\n' + prompt}]\n",
    "    messages += [{'role': 'assistant', 'content': completion}]\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c787efeb-2dd6-4794-a149-a1f499df384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ms = train.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "val_ms = val.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "test_ms = test.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8ac1a7e2-461d-46a9-9327-46fcca0610d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(train_ms).to_json('./data/openai/texner-finetune-name.train.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(val_ms).to_json('./data/openai/texner-finetune-name.val.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(test_ms).to_json('./data/openai/texner-finetune-name.test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "48693f5d-3711-4bd8-a9e0-c29b04f2fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "No errors found\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "format_error_check(train_ms)\n",
    "format_error_check(val_ms)\n",
    "format_error_check(test_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9d13ac7d-69de-4510-aa87-2a9740309456",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 62, 1109\n",
      "mean / median: 896.9734513274336, 999.0\n",
      "p5 / p95: 422.0, 1048.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 7, 560\n",
      "mean / median: 430.9867256637168, 481.0\n",
      "p5 / p95: 188.0, 513.5\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~202716 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~608148 tokens\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in train_ms:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(train_ms)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71ce7261-2cb1-4a50-adb5-9ae318ff0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "train_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune-name.train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "val_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune-name.val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "test_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune-name.test.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a638ec82-c3e4-4f94-abba-8ae486b578fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\",\n",
    "  validation_file=val_file.id,\n",
    "  seed=1299874447,\n",
    "  suffix=\"tex-ner-name\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "53c486bd-2049-401e-a260-090b48960670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-BC5RHeiInVKLtMyXDZL5cUEo', created_at=1732046905, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp:name-only:AVPFZfuJ', finished_at=1732048591, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=1.8), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-WEn9XU2tXbsjLzHzQeX2qlo4', result_files=['file-b3RW1iWMcLsKPufb1fzcpbd8'], seed=1299874447, status='succeeded', trained_tokens=249855, training_file='file-TaEfXKr11UrdrOzPITVora54', validation_file='file-5AmsSrh2cILvcO5PauKwb29Y', estimated_finish=None, integrations=[], user_provided_suffix='name-only')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4676f79-8522-48bd-908b-78efcbeb2d71",
   "metadata": {},
   "source": [
    "# Name/Ref Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3701da7a-69d3-4eea-a00f-fe8643254798",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "for file in os.listdir('./data/roberta-base-refs/train/'):\n",
    "    trains.extend(convert_iob_to_xml_name_ref(f'./data/roberta-base-refs/train/{file}', tokenizer, 512))\n",
    "\n",
    "vals = []\n",
    "for file in os.listdir('./data/roberta-base-refs/val/'):\n",
    "    vals.extend(convert_iob_to_xml_name_ref(f'./data/roberta-base-refs/val/{file}', tokenizer, 512))\n",
    "\n",
    "test = []\n",
    "for file in os.listdir('./data/roberta-base-refs/test/'):\n",
    "    test.extend(convert_iob_to_xml_name_ref(f'./data/roberta-base-refs/test/{file}', tokenizer, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "16514b77-07ba-45b2-a8fd-f2c4f6b0f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_and_ref_examples(annotations, tags: tuple[str] = ('name', 'reference'), force_references: bool = False):\n",
    "    df = pd.DataFrame.from_records(annotations)\n",
    "    df['length'] = df.end - df.start\n",
    "    names_and_refs = df[df.tag.isin(tags)].copy()\n",
    "    all_parents = []\n",
    "    for i, row in names_and_refs.iterrows():\n",
    "        parents = df[(df.start <= row.start) & (df.end >= row.end) & df.tag.isin({'theorem', 'definition', 'example'})]\n",
    "        if len(parents) > 0:\n",
    "            parent = parents.sort_values('length').iloc[0]\n",
    "            d = parent.to_dict()\n",
    "            d['child'] = row.annoid\n",
    "            all_parents.append(d)\n",
    "    \n",
    "    names_and_refs = pd.merge(left=names_and_refs, right=pd.DataFrame.from_records(all_parents), left_on='annoid', right_on='child', suffixes=('', '_parent'))\n",
    "    examples = []\n",
    "    for i, ex in names_and_refs.groupby('annoid_parent').agg(list).iterrows():\n",
    "        if force_references and 'reference' not in ex.tag:\n",
    "            continue\n",
    "        text = ex.text_parent[0]\n",
    "        offset = ex.start_parent[0]\n",
    "        parent_end = ex.end_parent[0]\n",
    "        completion = []\n",
    "        indices = sorted(zip(ex.start, ex.end, ex.tag))\n",
    "        for (ref_start, ref_end, tag), (next_start, next_end, _) in zip(indices, indices[1:] + [(parent_end, parent_end, None)]):\n",
    "            start = ref_start - offset\n",
    "            end = ref_end - offset\n",
    "            filler_end = next_start - offset\n",
    "            completion.append(f\"<{tag}>\")\n",
    "            completion.append(text[start:end])\n",
    "            completion.append(f\"</{tag}>\")\n",
    "            completion.append(text[end:filler_end])\n",
    "            \n",
    "        example = {'prompt': text, 'completion': ''.join(completion)}\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9f2b10dd-c9a0-4d2b-917d-6a252ac29f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example_message(prompt, completion, instruction, system):\n",
    "    messages = [{'role': 'system', 'content': system}]\n",
    "    messages += [{'role': 'user', 'content': instruction + '\\n\\n' + prompt}]\n",
    "    messages += [{'role': 'assistant', 'content': completion}]\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "86f954a0-08db-45d8-ba29-c582668688f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_and_ref_dataset(base_dir: str, output_dir: str, name_only: bool):\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    SYST = \"You are an expert mathematician who is fluent in reading LaTeX and extracting information.\"\n",
    "    if name_only:\n",
    "        INST = \"In the following LaTeX snippet, extract every newly defined named entity (name). Your output should be in XML.\"\n",
    "    else:\n",
    "        INST = \"In the following LaTeX snippet, extract every newly defined named entity (name) and reference to previously defined named entity (reference). Your output should be in XML.\"\n",
    "\n",
    "    trains = []\n",
    "    tags = ('name',) + (('reference',) if not name_only else ())\n",
    "    for file in os.listdir(f\"{base_dir}/train/\"):\n",
    "        with open(f'{base_dir}/train/{file}', 'r') as f:\n",
    "            annos = json.load(f)['annotations']\n",
    "            trains.extend(extract_name_and_ref_examples(annos, tags=tags, force_references=not name_only))\n",
    "    \n",
    "    vals = []\n",
    "    for file in os.listdir(f\"{base_dir}/val/\"):\n",
    "        with open(f'{base_dir}/val/{file}', 'r') as f:\n",
    "            annos = json.load(f)['annotations']\n",
    "            vals.extend(extract_name_and_ref_examples(annos, tags=tags, force_references=not name_only))\n",
    "    test = []\n",
    "    for file in os.listdir(f\"{base_dir}/test/\"):\n",
    "        with open(f'{base_dir}/test/{file}', 'r') as f:\n",
    "            annos = json.load(f)['annotations']\n",
    "            test.extend(extract_name_and_ref_examples(annos, tags=tags, force_references=not name_only))\n",
    "    train = pd.DataFrame(trains)\n",
    "    val = pd.DataFrame(vals)\n",
    "    test = pd.DataFrame(test)\n",
    "\n",
    "    train_ms = train.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "    val_ms = val.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "    test_ms = test.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "    pd.DataFrame.from_records(train_ms).to_json(f'{output_dir}/train.jsonl', orient='records', lines=True)\n",
    "    pd.DataFrame.from_records(val_ms).to_json(f'{output_dir}/val.jsonl', orient='records', lines=True)\n",
    "    pd.DataFrame.from_records(test_ms).to_json(f'{output_dir}/test.jsonl', orient='records', lines=True)\n",
    "    \n",
    "    return train_ms, val_ms, test_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6ca25845-eb47-4480-9068-12e8bae8f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(data_dir: str, job_name: str, base_model: str = \"gpt-4o-mini-2024-07-18\"):\n",
    "    client = OpenAI()\n",
    "    train_file = client.files.create(\n",
    "      file=open(f\"{data_dir}/train.jsonl\", \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "    val_file = client.files.create(\n",
    "      file=open(f\"{data_dir}/val.jsonl\", \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "    test_file = client.files.create(\n",
    "      file=open(f\"{data_dir}/test.jsonl\", \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "    format_error_check(train)\n",
    "    format_error_check(val)\n",
    "    format_error_check(test)\n",
    "    job = client.fine_tuning.jobs.create(\n",
    "      training_file=train_file.id,\n",
    "      model=base_model,\n",
    "      validation_file=val_file.id,\n",
    "      seed=1299874447,\n",
    "      suffix=job_name,\n",
    "    )\n",
    "    return job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "08714994-5729-4ead-8b80-2836141026d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt(test_data: list[dict], model: str):\n",
    "    preds = []\n",
    "    for messages in tqdm(test_data):\n",
    "        system, user, assistant = messages['messages']\n",
    "        completion = client.chat.completions.create(\n",
    "          model=model,\n",
    "          messages=[system, user],\n",
    "        )\n",
    "        preds.append(completion.choices[0].message.content)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7913b431-0b81-4197-96c2-32ad87da705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_name_and_ref_dataset('./data/roberta-base-new-data/', output_dir='./data/openai/name-only/', name_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6c30cfd8-5e26-4c1e-9f63-5ec11a222c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 24 18\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "86428f15-55ac-47f3-9d58-90f6454fab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "No errors found\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "job = train_gpt('./data/openai/name-only/', job_name='name-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d223cd22-9c4e-4f57-8891-a379e56aef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = create_name_and_ref_dataset('./data/roberta-base-refs/', output_dir='./data/openai/name-and-ref/', name_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a72f4ff4-9ee3-4b81-bbaa-3d3f09926c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "No errors found\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "NAME_ONLY_MODEL = \"ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp:name-only:AVPFZfuJ\"\n",
    "job = train_gpt('./data/openai/name-and-ref', job_name='name-and-ref', base_model=NAME_ONLY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "49321920-3196-4b92-86c0-2ad9ffd38c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while (job := client.fine_tuning.jobs.retrieve(job.id)).status == 'running':\n",
    "    time.sleep(5)\n",
    "print('All done!')\n",
    "NAME_REF_MODEL = job.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "873abec1-15bf-4cdd-91db-d1954c5c0d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:26<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = test_gpt(val, NAME_REF_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "dfb63621-6fa5-49f3-b716-1318cef1eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<name>cyclic groups</name> \\({\\bf Z}/n{\\bf Z}\\). Here our initial <reference>group</reference> is the <reference>integers</reference> \\({\\bf Z}\\) and our <reference>subgroup</reference> consists of all the <reference>multiples</reference> of some fixed <reference>integer</reference> \\(n\\):\n",
      "\n",
      "\\[n{<reference>\\bf Z</reference>}=\\{nk:k\\in{\\bf Z}\\}.\\]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Theorem 11.1.1</name>**: _Let \\(H\\) be a <reference>subgroup</reference> of \\(G\\). The set of <reference>cosets</reference> \\(gH\\), under the <reference>binary operation</reference>_\n",
      "\n",
      "\\[gH\\cdot\\hat{g}H=g\\hat{g}H,\\]\n",
      "\n",
      "_will form a <reference>group</reference> if and only if \\(H\\) is a <reference>normal subgroup</reference>. (This group is denoted by \\(G/H\\) and pronounced \\(G\\) mod \\(H\\).)_\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<reference>integers</reference> \\({\\bf Z}\\) under <reference>addition</reference> form an <reference>abelian group</reference>.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Definition 11.1.3</name>**: _A <reference>nonempty subset</reference> \\(H\\) of \\(<reference>G</reference>\\) is a <name>subgroup</name> if \\(H\\) is itself a <reference>group</reference>, using the <reference>binary operation</reference> of \\(G\\)._\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Definition 11.1.4</name>**: _Let \\(G\\) and \\(\\hat{G}\\) be two <reference>groups</reference>. Then a <reference>function</reference>_\n",
      "\n",
      "\\[\\sigma:G\\to\\hat{G}\\]\n",
      "\n",
      "_is a <name>group homomorphism</name> if for all \\(g_{1},g_{2}\\in G\\),_\n",
      "\n",
      "\\[\\sigma(g_{1}\\cdot g_{2})=\\sigma(g_{1})\\cdot\\sigma(g_{2}).\\]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Definition 11.1.5</name>**: _Let \\(H\\) be a <reference>subgroup</reference> of \\(G\\). The (left) <name>cosets</name> of \\(G\\) are all <reference>sets</reference> of the form_\n",
      "\n",
      "\\[gH=\\{gh:h\\in H\\},\\]\n",
      "\n",
      "_for \\(g\\in G\\)._This defines an <reference>equivalence class</reference> on \\(G\\), with\n",
      "\n",
      "\\[g\\sim\\hat{g}\\]\n",
      "\n",
      "if the set \\(gH\\) is equal to the set \\(\\hat{g}H\\), i.e., if there is an \\(h\\in H\\) with \\(gh=\\hat{g}\\). In a natural way, the _<name>right cosets</name>_ are the sets\n",
      "\n",
      "\\[Hg=\\{hg:h\\in H\\},\\]\n",
      "\n",
      "which also define an <reference>equivalence relation</reference> on the <reference>group</reference> \\(G\\).\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>gH\\cdot\\hat{g}H</name>=<reference>g</reference>\\hat{<reference>g</reference>}H,\\]\n",
      "\n",
      "_will form a <reference>group</reference> if and only if \\(H\\) is a <reference>normal subgroup</reference>. (This group is denoted by \\(G/H\\) and pronounced \\(G\\) mod \\(H\\).)_\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<reference>GL(3,R)</reference>} of <reference>invertible</reference> \\(3\\times 3\\) matrices.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Definition 11.1.2</name>**: _A <reference>group</reference> that is <reference>commutative</reference> is <name>abelian</name>._\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Definition 11.1.6</name>**: _A <reference>subgroup</reference> \\(H\\) is <name>normal</name> if for all \\(g\\) in \\(G\\), \\(gHg^{\\sim 1}=H\\)._\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<reference>GL(n,R)</reference>} is a <reference>group of invertible matrices</reference> over a <reference>field</reference> \\(R\\). We define an <name>automorphism</name> \\(\\sigma\\) of this group by\n",
      "\n",
      "\\[\\sigma(B)=A^{-1}BA,\\]\n",
      "\n",
      "where \\(A\\) is a fixed matrix in \\(<reference>GL(n,R)</reference>\\).\n",
      "\n",
      "Now, for any two matrices \\(B,C\\in{<reference>GL(n,R)</reference>}\\), we have\n",
      "\n",
      "\\[\\sigma(BC)=A^{-1}BCA\\]\n",
      "\n",
      "\\[=A^{-1}BAA^{-1}CA\\]\n",
      "\n",
      "\\[=\\sigma(B)\\cdot\\sigma(C).\\]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>Lemma 1.24</name>: _Let \\(A\\) be a <reference>central simple algebra</reference>, and let \\(a_{i},\\,b_{i}\\in A\\) be such that \\(\\sum_{i=1}^{n}L_{a_{i}}R_{b_{i}}=0\\). If the \\(a_{i}\\)'s are <reference>linearly independent</reference>, then each \\(b_{i}=0\\). Similarly, if the \\(b_{i}\\)'s are linearly independent, then each \\(a_{i}=0\\)._\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "<name>A</name>\\) be an <reference>algebra</reference>. For \\(a,\\,b\\in A\\) we define <reference>maps</reference> \\(L_{a},R_{b}:A\\to A\\), called the **<name>left multiplication map</name>** and **<name>right multiplication map</name>**, by\n",
      "\n",
      "\\[L_{a}(x):=ax,\\ \\ R_{b}(x):=xb.\\]\n"
     ]
    }
   ],
   "source": [
    "print(*preds, sep='\\n\\n' + '-' * 100 + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d1d3070c-1669-4092-988f-b35eda4dc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_annotations(text, xml):\n",
    "    pattern = r\"<name>(?P<name>.*?)</name>|<reference>(?P<reference>.*?)</reference>\"\n",
    "    tokens = tokenizer(text)\n",
    "    offset = 0\n",
    "    # [(start, end, item), ...]\n",
    "    tags = []\n",
    "    for item in re.finditer(pattern, xml):\n",
    "        if item.group(\"name\") is not None:\n",
    "            start = item.start() - offset\n",
    "            end = start + len(item.group(\"name\"))\n",
    "            entity = item.group(\"name\")\n",
    "            tag = \"name\"\n",
    "            offset += (item.end() - item.start()) - len(item.group(\"name\"))\n",
    "        elif item.group(\"reference\") is not None:\n",
    "            start = item.start() - offset\n",
    "            end = start + len(item.group(\"reference\"))\n",
    "            entity = item.group(\"reference\")\n",
    "            tag = \"reference\"\n",
    "            offset += (item.end() - item.start()) - len(item.group(\"reference\"))\n",
    "        tags.append(dict(start=start,end=end,tag=tag,text=entity))\n",
    "    \n",
    "    iob_tags = [\"O\" for _ in tokens['input_ids']]\n",
    "    for row in tags:\n",
    "        entity = row['text']\n",
    "        tag = row['tag']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        spaces = len(entity) - len(entity.strip())\n",
    "        t_start = tokens.char_to_token(start) or tokens.char_to_token(start+spaces)\n",
    "        t_end = tokens.char_to_token(end-1) # -1 because the char end goes past the word\n",
    "        iob_tags[t_start] = f\"B-{tag}\"\n",
    "        for idx in range(t_start+1, t_end+1):\n",
    "            iob_tags[idx] = f\"I-{tag}\"\n",
    "    return tags, {'tokens': tokens['input_ids'], 'tags': iob_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92fedc1f-a82d-4335-94d9-8834c9e71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = df.apply(lambda row: extract_annotations(row.prompt, row.predictions)[0], axis=1)\n",
    "iob_tags = df.apply(lambda row: extract_annotations(row.prompt, row.predictions)[1], axis=1)\n",
    "gold_iob_tags = df.apply(lambda row: extract_annotations(row.prompt, row.completion)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72085243-f69a-4291-97c0-2df4596286bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'tokens': [0, 27728, 14, 44128, 48377, 37457,...\n",
       "1    {'tokens': [0, 30, 50118, 50118, 37457, 10975,...\n",
       "2    {'tokens': [0, 50118, 50118, 37457, 10975, 164...\n",
       "3    {'tokens': [0, 48134, 195, 19268, 2911, 14086,...\n",
       "4    {'tokens': [0, 9, 49783, 40051, 22900, 45152, ...\n",
       "5    {'tokens': [0, 37457, 322, 12512, 6, 49279, 50...\n",
       "6    {'tokens': [0, 49747, 282, 24303, 5214, 134, 3...\n",
       "7    {'tokens': [0, 49712, 322, 9068, 2], 'tags': [...\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_iob_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c019f42d-d01b-4245-a629-ba221da478f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3548387096774194"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(gold_iob_tags.apply(lambda x: x['tags']).tolist(), iob_tags.apply(lambda x: x['tags']).tolist(), mode=None, average='micro', scheme=IOB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "daf8e87a-4236-47a3-89e0-4abcd836e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "1    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "2    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "3    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "4    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "5    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "6    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "7                                      [O, O, O, O, O]\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_iob_tags.apply(lambda x: x['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8bc91d73-c04f-48ae-bbda-e386fbb535a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition 11.1.2 [name], (192,209)\n",
      " abelian [name], (244,252)\n",
      "Definition 11.1.3 [name], (567,584)\n",
      " nonempty subset [name], (590,606)\n",
      " subgroup [name], (626,635)\n",
      " group [reference], (656,662)\n",
      " binary operation [reference], (673,690)\n",
      "Definition 11.1.4 [name], (1014,1031)\n",
      " group homomorphism [name], (1124,1143)\n"
     ]
    }
   ],
   "source": [
    "for start, end, entity, tag in tags:\n",
    "    print(entity, f\"[{tag}], ({start},{end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2cd20-d6a6-4d77-8a7e-2abb509e92f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
