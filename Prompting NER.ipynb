{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8c25997b-c3f5-4b88-b661-2820ab26662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import openai\n",
    "import bs4\n",
    "from more_itertools import chunked, flatten\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.scheme import IOB2\n",
    "openai.api_key = open('/Users/offendo/.openai-key', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cfcd0-0c20-41e0-a267-68f60d048d54",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efdc2c7-800a-4a8d-9b64-81eb591b774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da8ab2d-d3d8-47b7-a433-06d9919f3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_xml(tokens, tags):\n",
    "    \"\"\"\n",
    "    Convert BIO tags to XML format as a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens (words).\n",
    "        tags (list): List of BIO tags corresponding to each token.\n",
    "        \n",
    "    Returns:\n",
    "        list: XML representation of the tagged entities as a list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    xml_tokens = []\n",
    "    current_entity = None\n",
    "    current_entity_tokens = []\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):  # Beginning of a new entity\n",
    "            # Close the previous entity tag if there is one\n",
    "            if current_entity:\n",
    "                xml_tokens.append(f\"<{current_entity}>\")\n",
    "                xml_tokens.extend(current_entity_tokens)\n",
    "                xml_tokens.append(f\"</{current_entity}>\")\n",
    "            # Start a new entity\n",
    "            current_entity = tag[2:]\n",
    "            current_entity_tokens = [token]\n",
    "        elif tag.startswith(\"I-\") and current_entity == tag[2:]:  # Inside the same entity\n",
    "            current_entity_tokens.append(token)\n",
    "        else:  # Outside any entity\n",
    "            # Close the previous entity tag if there is one\n",
    "            if current_entity:\n",
    "                xml_tokens.append(f\"<{current_entity}>\")\n",
    "                xml_tokens.extend(current_entity_tokens)\n",
    "                xml_tokens.append(f\"</{current_entity}>\")\n",
    "                current_entity = None\n",
    "                current_entity_tokens = []\n",
    "            # Add the token outside any tag\n",
    "            xml_tokens.append(token)\n",
    "\n",
    "    # Close any remaining entity tag at the end\n",
    "    if current_entity:\n",
    "        xml_tokens.append(f\"<{current_entity}>\")\n",
    "        xml_tokens.extend(current_entity_tokens)\n",
    "        xml_tokens.append(f\"</{current_entity}>\")\n",
    "        \n",
    "    return xml_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a7b73f-508c-4297-94f0-771ce0aff76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_iob_to_xml(json_file, tokenizer):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    labels = ['definition', 'theorem', 'example', 'proof']\n",
    "    tokens = []\n",
    "    joined_tags = []\n",
    "    for token, tags in data['iob_tags']:\n",
    "        filtered_tags = [t.replace('B-', '').replace('I-', '') for t in tags]\n",
    "        filtered_tags = [t for t in filtered_tags if t in labels]\n",
    "        begin = any([t.startswith('B-') for t in tags if t.strip('B').strip('I').strip('-') in labels])\n",
    "        joined = '-'.join(sorted(filtered_tags)) or 'O'\n",
    "        if joined != 'O':\n",
    "            joined = 'B-' + joined if begin else 'I-' + joined\n",
    "        if len(joined_tags) > 1 and joined.replace('B-', '') == joined_tags[-1].replace('B-', '').replace('I-', ''):\n",
    "            joined = joined.replace('B-', 'I-')\n",
    "        joined_tags.append(joined)\n",
    "        tokens.append(token)\n",
    "\n",
    "    xml_result = bio_to_xml(tokens, joined_tags)\n",
    "    inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "    response = tokenizer.convert_tokens_to_string(xml_result)\n",
    "    return {'prompt': inputs, 'completion': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d674f4-d047-42a5-a69d-578ff63eeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_entities(tokens, tags, tokenizer):\n",
    "    entities = {}\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            # Start of a new entity\n",
    "            if current_entity and current_label:\n",
    "                # Add the previous entity to the result\n",
    "                entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "                entities.setdefault(current_label, []).append(entity_str)\n",
    "            current_entity = [token]\n",
    "            current_label = tag[2:]\n",
    "        elif tag.startswith(\"I-\") and current_label == tag[2:]:\n",
    "            # Continuation of the current entity\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            # End of an entity\n",
    "            if current_entity and current_label:\n",
    "                entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "                entities.setdefault(current_label, []).append(entity_str)\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "            if tag.startswith(\"B-\"):\n",
    "                current_entity = [token]\n",
    "                current_label = tag[2:]\n",
    "\n",
    "    # Add the last entity if there is one\n",
    "    if current_entity and current_label:\n",
    "        entity_str = tokenizer.convert_tokens_to_string(current_entity)\n",
    "        entities.setdefault(current_label, []).append(entity_str)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea011d5-c938-4752-98a7-931c96bfdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_iob_to_json(json_file, tokenizer):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = []\n",
    "    for chunk in chunked(data['iob_tags'], n=512):\n",
    "        entities = dict(definition=[], theorem=[], example=[], proof=[])\n",
    "        current_def = []\n",
    "        current_theorem = []\n",
    "        current_example = []\n",
    "        current_proof = []\n",
    "        tokens = []\n",
    "        for token, tags in chunk:\n",
    "            tokens.append(token)\n",
    "            for tag in tags:\n",
    "                if tag.startswith('B-') and 'definition' in tag:\n",
    "                    if current_def:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_def)\n",
    "                        entities['definition'].append(entity_str)\n",
    "                    current_def = [token]\n",
    "                if tag.startswith('I-') and 'definition' in tag:\n",
    "                    current_def.append(token)\n",
    "                    \n",
    "                if tag.startswith('B-') and 'theorem' in tag:\n",
    "                    if current_theorem:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_theorem)\n",
    "                        entities['theorem'].append(entity_str)\n",
    "                    current_theorem = [token]\n",
    "                if tag.startswith('I-') and 'theorem' in tag:\n",
    "                    current_theorem.append(token)\n",
    "                    \n",
    "                if tag.startswith('B-') and 'example' in tag:\n",
    "                    if current_example:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_example)\n",
    "                        entities['example'].append(entity_str)\n",
    "                    current_example = [token]\n",
    "                if tag.startswith('I-') and 'example' in tag:\n",
    "                    current_example.append(token)    \n",
    "                    \n",
    "                if tag.startswith('B-') and 'proof' in tag:\n",
    "                    if current_proof:\n",
    "                        entity_str = tokenizer.convert_tokens_to_string(current_proof)\n",
    "                        entities['proof'].append(entity_str)\n",
    "                    current_proof = [token]\n",
    "                if tag.startswith('I-') and 'proof' in tag:\n",
    "                    current_proof.append(token)\n",
    "    \n",
    "        if current_def:\n",
    "            entities['definition'].append(tokenizer.convert_tokens_to_string(current_def))\n",
    "        if current_theorem:\n",
    "            entities['theorem'].append(tokenizer.convert_tokens_to_string(current_theorem))\n",
    "        if current_example:\n",
    "            entities['example'].append(tokenizer.convert_tokens_to_string(current_example))\n",
    "        if current_proof:\n",
    "            entities['proof'].append(tokenizer.convert_tokens_to_string(current_proof))\n",
    "            \n",
    "        inputs = tokenizer.convert_tokens_to_string(tokens)\n",
    "        response = json.dumps(entities)\n",
    "        example = {'prompt': inputs, 'completion': response}\n",
    "        examples.append(example)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaff0f45-2067-458b-9446-bd7f9b693b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/train/'):\n",
    "    trains.extend(convert_iob_to_json(f'./data/roberta-base-new-data/train/{file}', tokenizer))\n",
    "\n",
    "vals = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/val/'):\n",
    "    vals.extend(convert_iob_to_json(f'./data/roberta-base-new-data/val/{file}', tokenizer))\n",
    "\n",
    "test = []\n",
    "for file in os.listdir('./data/roberta-base-new-data/test/'):\n",
    "    test.extend(convert_iob_to_json(f'./data/roberta-base-new-data/test/{file}', tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f69698d4-8626-415c-aea2-621192341bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(trains)\n",
    "val = pd.DataFrame(vals)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "train.to_json('./data/openai/train.jsonl', orient='records', lines=True)\n",
    "val.to_json('./data/openai/val.jsonl', orient='records', lines=True)\n",
    "test.to_json('./data/openai/test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c06f1409-0130-4772-9e2c-74abadbb5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_evaluate(golds: list[dict[str, list[str]]], preds: list[dict[str, list[str]]]):\n",
    "    true_pos = {}\n",
    "    false_pos = {}\n",
    "    false_neg = {}\n",
    "    prec = {}\n",
    "    rec = {}\n",
    "    f1 = {}\n",
    "    for label in ['definition', 'theorem', 'proof', 'example']:\n",
    "        gold = [x for g in golds for x in g[label]]\n",
    "        pred = [x for p in preds for x in p[label]]\n",
    "        true_pos[label] = len([p for p in pred if p in gold])\n",
    "        false_pos[label] = len([p for p in pred if p not in gold])\n",
    "        false_neg[label] = len([g for g in gold if g not in pred])\n",
    "        prec[label] = true_pos[label] / (true_pos[label] + false_pos[label])\n",
    "        rec[label] = true_pos[label] / (true_pos[label] + false_neg[label])\n",
    "        f1[label] = 2 * prec[label] * rec[label] / (prec[label] + rec[label])\n",
    "\n",
    "    macro_f1 = sum(f1.values()) / len(f1.values())\n",
    "    tp = sum(true_pos.values())\n",
    "    fp = sum(false_pos.values())\n",
    "    fn = sum(false_neg.values())\n",
    "    p = tp / (tp + fp)\n",
    "    r = tp / (tp + fn)\n",
    "    micro_f1 = 2 * p * r / (p + r)\n",
    "    return {'micro_f1': micro_f1, 'macro_f1': macro_f1, 'f1': f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56271b1-aa42-4944-83a7-c4997cf5b3ff",
   "metadata": {},
   "source": [
    "# Few-shot Prompting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1544d203-93aa-4392-b5f9-092e069f96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bio_tags(sentence, entities):\n",
    "    # Tokenize the sentence by splitting on spaces and punctuation, while keeping the tokens intact\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence, re.UNICODE)\n",
    "    tags = [[] for _ in range(len(tokens))]\n",
    "\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        for entity in entity_list:\n",
    "            # Tokenize the entity name in the same way to match against tokens\n",
    "            entity_tokens = re.findall(r\"\\w+|[^\\w\\s]\", entity, re.UNICODE)\n",
    "            entity_len = len(entity_tokens)\n",
    "            # Search for the entity tokens in the sentence tokens\n",
    "            for i in range(len(tokens) - entity_len + 1):\n",
    "                if tokens[i:i + entity_len] == entity_tokens:\n",
    "                    for j in range(entity_len):\n",
    "                        tags[i + j].append(f\"{entity_type}\")\n",
    "                    break  # Move to the next entity once found\n",
    "    joined_tags = ['-'.join(sorted(set(t))) if len(t) > 0 else 'O' for t in tags]\n",
    "    return tokens, joined_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f6f348d-1e74-4435-b602-dd662744f27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_model_output(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df['predictions'] = df['predictions'].apply(lambda x: json.loads(x))\n",
    "    df['completion'] = df['completion'].apply(lambda x: json.loads(x.replace(eos, \"\")))\n",
    "    preds = {}\n",
    "    for p in df.predictions:\n",
    "        for k in p.keys():\n",
    "            preds[k] = preds.get(k, []) + p[k]\n",
    "    golds = {}\n",
    "    for g in df.completion:\n",
    "        for k in g.keys():\n",
    "            golds[k] = golds.get(k, []) + g[k]\n",
    "    tokens, gold_tags = generate_bio_tags(' '.join(df.prompt), golds)\n",
    "    tokens, pred_tags = generate_bio_tags(' '.join(df.prompt), preds)\n",
    "    labels = [t for t in set(gold_tags) if t != 'O']\n",
    "    p, r, f, _ = precision_recall_fscore_support(gold_tags, pred_tags, average=None, labels=labels)\n",
    "    mp, mr, mf, _ = precision_recall_fscore_support(gold_tags, pred_tags, average='micro', labels=labels)\n",
    "\n",
    "    result = pd.DataFrame({'tokens': tokens, 'golds': gold_tags, 'preds': pred_tags})\n",
    "    metrics = dict(\n",
    "        precision=dict(zip(labels, p)),\n",
    "        recall=dict(zip(labels, r)),\n",
    "        f1=dict(zip(labels, f)),\n",
    "        micro_precision=mp,\n",
    "        micro_recall=mr,\n",
    "        micro_f1=mf,\n",
    "    )\n",
    "    return metrics, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f35eb9ff-18f5-4caf-8404-706a1fc457f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.13625,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.7440476190476191,\n",
      "        'proof': 0.46064889174429813,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.14026402640264027},\n",
      " 'micro_f1': 0.304661079253051,\n",
      " 'micro_precision': 0.7754491017964071,\n",
      " 'micro_recall': 0.18956999085086917,\n",
      " 'precision': {'definition': 1.0,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.6157635467980296,\n",
      "               'proof': 0.8056179775280898,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6343283582089553},\n",
      " 'recall': {'definition': 0.07310529845741114,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.9398496240601504,\n",
      "            'proof': 0.3225371120107962,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.07884972170686456}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-3.5-turbo.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "77ab2d69-201b-411d-8662-100a71305efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.42953472690492245,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.8632478632478633,\n",
      "        'proof': 0.7035126504544338,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.4852492370295015},\n",
      " 'micro_f1': 0.541474890048072,\n",
      " 'micro_precision': 0.6138682745825603,\n",
      " 'micro_recall': 0.48435498627630375,\n",
      " 'precision': {'definition': 0.43186440677966104,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.7748917748917749,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.5371621621621622},\n",
      " 'recall': {'definition': 0.4272300469483568,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.7593984962406015,\n",
      "            'proof': 0.644174538911381,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.4424860853432282}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4-turbo.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "748fcfbe-d5ad-4057-95a0-058a4c053535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5944418276024493,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.0,\n",
      "        'proof': 0.3002645502645503,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.38411910669975186},\n",
      " 'micro_f1': 0.3757498404594767,\n",
      " 'micro_precision': 0.6210970464135022,\n",
      " 'micro_recall': 0.26935041171088747,\n",
      " 'precision': {'definition': 0.9984177215189873,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.0,\n",
      "               'proof': 0.5667915106117354,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.41302027748132336},\n",
      " 'recall': {'definition': 0.4232059020791415,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.0,\n",
      "            'proof': 0.20422852001799371,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.3589981447124304}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4o-mini.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f25b0b5-4681-4f22-86b7-98ba62753643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.6274199920979849,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.5859030837004405,\n",
      "        'proof': 0.6983471074380165,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.49830508474576274},\n",
      " 'micro_f1': 0.5934329660739609,\n",
      " 'micro_precision': 0.7347379794705564,\n",
      " 'micro_recall': 0.4977127172918573,\n",
      " 'precision': {'definition': 0.7634615384615384,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.4143302180685358,\n",
      "               'proof': 0.8198908429351122,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6372832369942196},\n",
      " 'recall': {'definition': 0.5325285043594903,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 1.0,\n",
      "            'proof': 0.6081871345029239,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.4090909090909091}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-gpt-4o.5shot.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dc45551d-ed66-49cc-9962-5f5e80c53abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5504241281809613,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.18815331010452963,\n",
      "        'proof': 0.6437793427230047,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.2773837667454689},\n",
      " 'micro_f1': 0.4940991345397325,\n",
      " 'micro_precision': 0.8718186024988431,\n",
      " 'micro_recall': 0.34473924977127174,\n",
      " 'precision': {'definition': 0.9255150554675119,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.17532467532467533,\n",
      "               'proof': 0.9257383966244725,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.9214659685863874},\n",
      " 'recall': {'definition': 0.39168343393695504,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.20300751879699247,\n",
      "            'proof': 0.49347728295096716,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.16326530612244897}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-mini.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c901e0d5-20de-461a-a157-32052579886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.1524609843937575,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.0,\n",
      "        'proof': 0.35690968443960824,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.23993558776167473},\n",
      " 'micro_f1': 0.2423477437677501,\n",
      " 'micro_precision': 0.8797250859106529,\n",
      " 'micro_recall': 0.14053064958828912,\n",
      " 'precision': {'definition': 0.7257142857142858,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.0,\n",
      "               'proof': 0.9213483146067416,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.9085365853658537},\n",
      " 'recall': {'definition': 0.085177733065057,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.0,\n",
      "            'proof': 0.2213225371120108,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.13821892393320964}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-mini.5shot.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "169dabf1-1f37-469d-8cd9-1dc1ea26b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.6172506738544474,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.7208672086720868,\n",
      "        'proof': 0.6985842985842986,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.46497584541062803},\n",
      " 'micro_f1': 0.5900506678949793,\n",
      " 'micro_precision': 0.7958993476234856,\n",
      " 'micro_recall': 0.4688014638609332,\n",
      " 'precision': {'definition': 0.9346938775510204,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 0.5635593220338984,\n",
      "               'proof': 0.8164861612515042,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.6660899653979239},\n",
      " 'recall': {'definition': 0.4607645875251509,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 1.0,\n",
      "            'proof': 0.6104363472784525,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.35714285714285715}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-preview.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "11acfdc0-4d46-4d18-92b3-9f82410b7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-o1-preview.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-o1-preview.val.outputs.csv > results/openai-o1-preview.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "76374de7-95a8-4b5c-a24f-28ff748c0cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5888704318936877,\n",
      "        'definition-theorem': 0.0,\n",
      "        'example': 0.3878787878787879,\n",
      "        'proof': 0.8414690841469085,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.6076086956521739},\n",
      " 'micro_f1': 0.664956168484071,\n",
      " 'micro_precision': 0.7996914373875033,\n",
      " 'micro_recall': 0.5690759377859104,\n",
      " 'precision': {'definition': 0.7731733914940022,\n",
      "               'definition-theorem': 0.0,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.8706108706108706,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.7335958005249343},\n",
      " 'recall': {'definition': 0.4755197853789403,\n",
      "            'definition-theorem': 0.0,\n",
      "            'example': 0.24060150375939848,\n",
      "            'proof': 0.8142150247413406,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.5185528756957328}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-o1-preview.5shot.val-fixed.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "658d8e67-ffc8-4a44-ba00-9c72c62944ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-o1-preview.5shot.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-o1-preview.5shot.val.outputs.csv > results/openai-o1-preview.5shot.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce8745-a314-4edf-aca4-33e9d20dd9bb",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2cb0a529-9020-4be1-b21a-398a586d6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "INST = \"\"\"In the following LaTeX document, extract entities of the following types and return them in JSON output.\n",
    "1. definition\n",
    "2. theorem\n",
    "3. proof\n",
    "4. example\n",
    "Your output should be a single JSON with 4 keys corresponding to the 4 entity types above. Spans may be part of multiple entities. Do not hallucinate text.\n",
    "\"\"\"\n",
    "SYST = \"You are an expert mathematician who is fluent in reading LaTeX and extracting information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "74e5d583-f54b-4a0b-90bf-0fe20d936a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example_message(prompt, completion, instruction, system):\n",
    "    messages = [{'role': 'system', 'content': system}]\n",
    "    messages += [{'role': 'user', 'content': instruction + '\\n\\n' + prompt}]\n",
    "    messages += [{'role': 'assistant', 'content': completion}]\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b73f73cc-1e42-402b-8b8a-4d93bbf16c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ms = train.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "val_ms = val.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()\n",
    "test_ms = test.apply(lambda row: make_example_message(row.prompt, row.completion, INST, SYST), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f2b4c3e-925e-46d1-bd8c-16830b2c6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(train_ms).to_json('./data/openai/texner-finetune.train.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(val_ms).to_json('./data/openai/texner-finetune.val.jsonl', orient='records', lines=True)\n",
    "pd.DataFrame.from_records(test_ms).to_json('./data/openai/texner-finetune.test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "561565cd-5eb1-4746-85fd-7deb5539d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_error_check(dataset):\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "    \n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "    \n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cef9f25c-8b3b-4a1f-9193-f318f5c5aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n",
      "No errors found\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "format_error_check(train_ms)\n",
    "format_error_check(val_ms)\n",
    "format_error_check(test_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9c0247f8-c6c6-4768-a254-481f1d6c8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 128, 1300\n",
      "mean / median: 925.6946902654868, 1021.0\n",
      "p5 / p95: 510.0, 1150.5\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 17, 702\n",
      "mean / median: 406.7079646017699, 462.0\n",
      "p5 / p95: 147.5, 568.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~209207 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~627621 tokens\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in train_ms:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(train_ms)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "db4bce8b-34a6-4072-acf0-7be5c657dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "train_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "val_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "test_file = client.files.create(\n",
    "  file=open(\"data/openai/texner-finetune.test.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf84474c-57c9-4d26-8757-365eeeeecb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-lGAIM9gRjdt3DXYtMJkqXLY0'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "31c08d1e-f0de-4393-9dc0-eda40f6d3011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-zZYkGXxi6xarW0MD9Sreegeb', created_at=1730767393, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-WEn9XU2tXbsjLzHzQeX2qlo4', result_files=[], seed=1299874447, status='validating_files', trained_tokens=None, training_file='file-lGAIM9gRjdt3DXYtMJkqXLY0', validation_file='file-SnIToERKbLAJyyde0RDssg6L', estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\",\n",
    "  validation_file=val_file.id,\n",
    "  seed=1299874447\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d93b6f76-c87b-46b7-92c1-3aa9c0fec1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ms) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "35890d06-c288-4fdc-8742-80c5dd9efb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.5320322443784472,\n",
      "        'definition-theorem': 0.39285714285714285,\n",
      "        'example': 0.9609375,\n",
      "        'proof': 0.8079616660523405,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.664367816091954},\n",
      " 'micro_f1': 0.6961357752421133,\n",
      " 'micro_precision': 0.7312651087832394,\n",
      " 'micro_recall': 0.6642268984446478,\n",
      " 'precision': {'definition': 0.7240184757505773,\n",
      "               'definition-theorem': 1.0,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.6843584139868873,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.8731117824773413},\n",
      " 'recall': {'definition': 0.42052313883299797,\n",
      "            'definition-theorem': 0.24444444444444444,\n",
      "            'example': 0.924812030075188,\n",
      "            'proof': 0.9860548807917229,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.536178107606679}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/offendo/src/autoformalization/ner/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# with open('./response_schema.json', 'r') as f:\n",
    "#     schema = json.load(f)\n",
    "# preds = []\n",
    "# for messages in tqdm(val_ms):\n",
    "#     system, user, assistant = messages['messages']\n",
    "#     completion = client.chat.completions.create(\n",
    "#       model=\"ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp::AQ2BxcPd\",\n",
    "#       messages=[system, user],\n",
    "#       response_format=schema,\n",
    "#     )\n",
    "#     preds.append(json.loads(completion.choices[0].message.content))\n",
    "# val['predictions'] = [json.dumps(p) for p in preds]\n",
    "val.to_json('./results/openai-4o-finetuned.val.jsonl', orient='records', lines=True)\n",
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.val.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a00cd5e7-2485-4cc2-aa43-130e38f14da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-4o-finetuned.val.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.val.outputs.csv > results/openai-4o-finetuned.val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc6d2871-83a0-4852-8b60-51aca80abd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:31<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for messages in tqdm(test_ms):\n",
    "    system, user, assistant = messages['messages']\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"ft:gpt-4o-mini-2024-07-18:uc-santa-cruz-jlab-nlp::AQ2BxcPd\",\n",
    "      messages=[system, user],\n",
    "      response_format=schema,\n",
    "    )\n",
    "    preds.append(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "31cd99dd-40d2-42c0-a6d1-49be6ad38c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.9069616135328562,\n",
      "        'example': 0.5784883720930233,\n",
      "        'proof': 0.9402356902356902,\n",
      "        'theorem': 0.7431869188842578},\n",
      " 'micro_f1': 0.8887585949576249,\n",
      " 'micro_precision': 0.898771021992238,\n",
      " 'micro_recall': 0.8789667896678967,\n",
      " 'precision': {'definition': 0.8653010552451893,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.9226829671237403,\n",
      "               'theorem': 0.8202406227883935},\n",
      " 'recall': {'definition': 0.9528366370471634,\n",
      "            'example': 0.4069529652351738,\n",
      "            'proof': 0.9584691951261369,\n",
      "            'theorem': 0.67936694021102}}\n"
     ]
    }
   ],
   "source": [
    "# test['predictions'] = preds\n",
    "test.to_json('./results/openai-4o-finetuned.test.jsonl', orient='records', lines=True)\n",
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.test.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7b8f458a-7a78-445d-b284-e4d7b9684968",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-4o-finetuned.test.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.test.outputs.csv > results/openai-4o-finetuned.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a84261cd-20ba-4374-9f5c-b71e814d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat './results/openai-4o-finetuned.test.jsonl' './results/openai-4o-finetuned.val.jsonl'  > './results/openai-4o-finetuned.both.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9a877db8-6785-45bc-a252-87c98bc7f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'definition': 0.744245995212668,\n",
      "        'definition-theorem': 0.2597402597402597,\n",
      "        'example': 0.6822033898305084,\n",
      "        'proof': 0.8991271171744032,\n",
      "        'proof-theorem': 0.0,\n",
      "        'theorem': 0.7154906397860522},\n",
      " 'micro_f1': 0.8119805976730776,\n",
      " 'micro_precision': 0.8233473206301163,\n",
      " 'micro_recall': 0.8009234475374732,\n",
      " 'precision': {'definition': 0.8159063383124747,\n",
      "               'definition-theorem': 0.2770780856423174,\n",
      "               'example': 1.0,\n",
      "               'proof': 0.8403025391680173,\n",
      "               'proof-theorem': 0.0,\n",
      "               'theorem': 0.8372652864708714},\n",
      " 'recall': {'definition': 0.6841570751523358,\n",
      "            'definition-theorem': 0.24444444444444444,\n",
      "            'example': 0.5176848874598071,\n",
      "            'proof': 0.9668075584286425,\n",
      "            'proof-theorem': 0.0,\n",
      "            'theorem': 0.6246408045977011}}\n"
     ]
    }
   ],
   "source": [
    "metrics, outputs = score_model_output('./results/openai-4o-finetuned.both.jsonl')\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "106a0b2c-2997-4789-9ade-a04b10cdb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.to_csv(f'results/openai-4o-finetuned.both.outputs.csv', sep='\\t', index=False)\n",
    "!column -t -s $'\\t' results/openai-4o-finetuned.both.outputs.csv > results/openai-4o-finetuned.both.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7331405f-f3b5-40d5-a18e-3d1c7527d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7c72bd8a-00c2-4b9c-ac89-7f2e304c626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "golds = []\n",
    "current = None\n",
    "for g in outputs.golds:\n",
    "    if g == 'O':\n",
    "        current = None\n",
    "        golds.append('O')\n",
    "    elif g == current:\n",
    "        golds.append(f'I-{g}')\n",
    "    else:\n",
    "        current = g\n",
    "        golds.append(f\"B-{g}\")\n",
    "\n",
    "preds = []\n",
    "current = None\n",
    "for p in outputs.preds:\n",
    "    if p == 'O':\n",
    "        current = None\n",
    "        preds.append('O')\n",
    "    elif p == current:\n",
    "        preds.append(f'I-{p}')\n",
    "    else:\n",
    "        current = p\n",
    "        preds.append(f\"B-{p}\")\n",
    "outputs['bio_golds'] = golds\n",
    "outputs['bio_preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "78b9fe1c-3db9-43db-8202-4ee6486d3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [chunk for chunk in chunked(outputs.bio_preds, n=1024)]\n",
    "gs = [chunk for chunk in chunked(outputs.bio_golds, n=1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0d75e3a4-3f7f-4ad8-b8bf-2340006399a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3849765258215963"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(gs, ps, mode=None, average='micro', scheme=IOB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875c5b5-1734-45d0-a9f9-5f36c9aa9683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
